import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from trl import SFTTrainer
import os

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙŠÙ†ÙŠØ²Ø± ---

# Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Hugging Face (Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ± Ù†Ø³Ø¨ÙŠØ§Ù‹ Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø©)
base_model_name = "mistralai/Mistral-7B-Instruct-v0.2"
# Ø§Ø³Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØµÙ‚ÙˆÙ„
output_dir = "./models/my-finetuned-model"

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Quantization Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (NF4)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

# --- 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙŠÙ†ÙŠØ²Ø± ---

print("â³ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙˆØ§Ù„ØªÙˆÙƒÙŠÙ†ÙŠØ²Ø±...")
model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map="auto"  # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ù€ GPU Ø§Ù„Ù…ØªØ§Ø­
)
# Ù…Ù†Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… cache Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
# Ø¥Ø¶Ø§ÙØ© padding token Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙŠÙ†ÙŠØ²Ø±.")

# --- 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---

print("â³ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
dataset = load_dataset("json", data_files="data/dataset.jsonl", split="train")
print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

# --- 4. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ---

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=1,              # Ø¹Ø¯Ø¯ Ø¯ÙˆØ±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (epoch)ØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ù€ 1
    per_device_train_batch_size=4,   # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ù„ÙƒÙ„ GPU
    gradient_accumulation_steps=1,   # Ø®Ø·ÙˆØ§Øª ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¯Ø±Ø¬
    learning_rate=2e-4,              # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    weight_decay=0.001,              # Ø§Ù†Ø­Ù„Ø§Ù„ Ø§Ù„ÙˆØ²Ù†
    optim="paged_adamw_32bit",       # Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†
    fp16=False,                      # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø©
    bf16=True,                       # Ø§Ø³ØªØ®Ø¯Ø§Ù… BF16 (Ù…ÙˆØµÙ‰ Ø¨Ù‡ Ù„Ù€ Ampere GPUs)
    max_grad_norm=0.3,               # Ø£Ù‚ØµÙ‰ Ù…Ø¹ÙŠØ§Ø± Ù„Ù„ØªØ¯Ø±Ø¬
    max_steps=-1,                    # Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª (ØªØ¬Ø§ÙˆØ² num_train_epochs)
    warmup_ratio=0.03,               # Ù†Ø³Ø¨Ø© Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¥Ø­Ù…Ø§Ø¡
    group_by_length=True,            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø·ÙˆÙ„
    lr_scheduler_type="constant",    # Ù†ÙˆØ¹ Ù…Ø¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    logging_steps=2,                 # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙƒÙ„ Ø®Ø·ÙˆØªÙŠÙ†
    save_steps=10,                   # Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ ÙƒÙ„ 10 Ø®Ø·ÙˆØ§Øª
    save_total_limit=2,              # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± Ù†Ù‚Ø·ØªÙŠ ØªÙØªÙŠØ´ ÙÙ‚Ø·
)

# --- 5. Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ¨Ø¯Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨ ---

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,  # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„Ù„ØªØ³Ù„Ø³Ù„
    tokenizer=tokenizer,
    args=training_args,
)

print("ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØµÙ‚Ù„ (Fine-Tuning)...")
trainer.train()
print("ğŸ‰ Ø§Ù†ØªÙ‡Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØµÙ‚Ù„ Ø¨Ù†Ø¬Ø§Ø­!")

# --- 6. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ---
print("ğŸ’¾ Ø¬Ø§Ø±Ù Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
final_model_path = os.path.join(output_dir, "final_checkpoint")
trainer.model.save_pretrained(final_model_path)
tokenizer.save_pretrained(final_model_path)
print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {final_model_path}")
