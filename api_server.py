from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import APIKeyHeader
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù† ÙˆÙ…ÙØªØ§Ø­ Ø§Ù„Ù€ API ---

# Ù‡Ø°Ø§ Ù‡Ùˆ Ù…ÙØªØ§Ø­ Ø§Ù„Ù€ API "Ø§Ù„Ø³Ø±ÙŠ" Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ. ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø­Ù‚ÙŠÙ‚ÙŠØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹ ÙˆÙŠÙØ¯Ø§Ø± Ø¨Ø£Ù…Ø§Ù†.
API_KEY = "MySuperSecret-API-Key-2024" 
API_KEY_NAME = "X-API-KEY"

api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

async def get_api_key(api_key: str = Depends(api_key_header)):
    if api_key == API_KEY:
        return api_key
    else:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Could not validate credentials"
        )

# --- 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØµÙ‚ÙˆÙ„ ---

app = FastAPI(
    title="API Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ù…Ø®ØµØµ",
    description="ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ù…ØµÙ‚ÙˆÙ„.",
    version="1.0.0"
)

model = None
tokenizer = None

@app.on_event("startup")
def load_model():
    global model, tokenizer
    model_path = "./models/my-finetuned-model/final_checkpoint"
    if not os.path.exists(model_path):
        raise RuntimeError("Model checkpoint not found. Please run finetune.py first.")
        
    print("â³ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØµÙ‚ÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©...")
    # Ø§Ø³ØªØ®Ø¯Ù… device_map="auto" Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ GPU
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        device_map="auto", 
        torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    print("âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!")

# --- 3. ØªØ¹Ø±ÙŠÙ Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoint) ---

class GenerationRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 150
    temperature: float = 0.7
    top_p: float = 0.95
    top_k: int = 50

class GenerationResponse(BaseModel):
    generated_text: str
    prompt: str

@app.post("/generate", response_model=GenerationResponse, dependencies=[Depends(get_api_key)])
async def generate_text(request: GenerationRequest):
    """
    ÙŠÙˆÙ„Ø¯ Ù†ØµØ§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©.
    """
    if not model or not tokenizer:
        raise HTTPException(status_code=503, detail="Model is not loaded yet.")

    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù€ prompt Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    formatted_prompt = f"<s>[INST] {request.prompt} [/INST]"
    
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    
    print("ğŸ¤–... Ø¬Ø§Ø±Ù ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_new_tokens,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,  # Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£ÙƒØ«Ø± Ø¥Ø¨Ø¯Ø§Ø¹Ù‹Ø§
            temperature=request.temperature,
            top_k=request.top_k,
            top_p=request.top_p,
        )
    
    # ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØªÙ†Ø¸ÙŠÙÙ‡Ø§
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù€ prompt Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    clean_response = response_text.replace(formatted_prompt, "").strip()
    
    return GenerationResponse(
        generated_text=clean_response,
        prompt=request.prompt
    )

@app.get("/health")
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None
    }

@app.get("/")
def read_root():
    return {
        "message": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ API Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ! ØªÙˆØ¬Ù‡ Ø¥Ù„Ù‰ /docs Ù„Ù„ØªÙˆØ«ÙŠÙ‚.",
        "docs_url": "/docs",
        "health_url": "/health"
    }
